<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-bounce.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"odddog-git.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Scrapy框架：写一个爬虫，需要做很多的事情。比如：发送网络请求、数据解析、数据存储、反反爬虫机制（更换ip代理、设置请求头等）、异步请求等。这些工作如果每次都要自己从零开始写的话，比较浪费时间。因此Scrapy把一些基础的东西封装好了，在他上面写爬虫可以变的更加的高效（爬取效率和开发效率）。因此真正在公司里，一些上了量的爬虫，都是使用Scrapy框架来解决。">
<meta property="og:type" content="article">
<meta property="og:title" content="scrapy框架">
<meta property="og:url" content="http://odddog-git.github.io/2021/04/16/scrapy%E6%A1%86%E6%9E%B6/index.html">
<meta property="og:site_name" content="hui&#39;s Blog">
<meta property="og:description" content="Scrapy框架：写一个爬虫，需要做很多的事情。比如：发送网络请求、数据解析、数据存储、反反爬虫机制（更换ip代理、设置请求头等）、异步请求等。这些工作如果每次都要自己从零开始写的话，比较浪费时间。因此Scrapy把一些基础的东西封装好了，在他上面写爬虫可以变的更加的高效（爬取效率和开发效率）。因此真正在公司里，一些上了量的爬虫，都是使用Scrapy框架来解决。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://z3.ax1x.com/2021/04/16/cRzQ1S.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/04/16/cRzl6g.png">
<meta property="article:published_time" content="2021-04-16T01:01:18.000Z">
<meta property="article:modified_time" content="2021-05-14T03:31:18.481Z">
<meta property="article:author" content="怪狗狗">
<meta property="article:tag" content="python">
<meta property="article:tag" content="scrapy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://z3.ax1x.com/2021/04/16/cRzQ1S.png">

<link rel="canonical" href="http://odddog-git.github.io/2021/04/16/scrapy%E6%A1%86%E6%9E%B6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>scrapy框架 | hui's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
  <a target="_blank" rel="noopener" href="https://oddDog-git" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">hui's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://odddog-git.github.io/2021/04/16/scrapy%E6%A1%86%E6%9E%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/jerry.jpg">
      <meta itemprop="name" content="怪狗狗">
      <meta itemprop="description" content="踏上新征程----go！！！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hui's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          scrapy框架
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-04-16 09:01:18" itemprop="dateCreated datePublished" datetime="2021-04-16T09:01:18+08:00">2021-04-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-14 11:31:18" itemprop="dateModified" datetime="2021-05-14T11:31:18+08:00">2021-05-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/04/16/scrapy%E6%A1%86%E6%9E%B6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/04/16/scrapy%E6%A1%86%E6%9E%B6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Scrapy框架："><a href="#Scrapy框架：" class="headerlink" title="Scrapy框架："></a>Scrapy框架：</h2><p>写一个爬虫，需要做很多的事情。比如：发送网络请求、数据解析、数据存储、反反爬虫机制（更换ip代理、设置请求头等）、异步请求等。这些工作如果每次都要自己从零开始写的话，比较浪费时间。因此Scrapy把一些基础的东西封装好了，在他上面写爬虫可以变的更加的高效（爬取效率和开发效率）。因此真正在公司里，一些上了量的爬虫，都是使用Scrapy框架来解决。</p>
<a id="more"></a>

<h3 id="安装Scrapy框架："><a href="#安装Scrapy框架：" class="headerlink" title="安装Scrapy框架："></a>安装Scrapy框架：</h3><ol>
<li>pip install scrapy</li>
</ol>
<h3 id="Scrapy框架架构："><a href="#Scrapy框架架构：" class="headerlink" title="Scrapy框架架构："></a>Scrapy框架架构：</h3><ol>
<li><p>Scrapy Engine（引擎）：Scrapy框架的核心部分。负责在Spider和ItemPipeline、Downloader、Scheduler中间通信、传递数据等。</p>
</li>
<li><p>Spider（爬虫）：发送需要爬取的链接给引擎，最后引擎把其他模块请求回来的数据再发送给爬虫，爬虫就去解析想要的数据。这个部分是我们开发者自己写的，因为要爬取哪些链接，页面中的哪些数据是需要的，都是由程序员自己决定。</p>
</li>
<li><p>Scheduler（调度器）：负责接收引擎发送过来的请求，并按照一定的方式进行排列和整理，负责调度请求的顺序等。</p>
</li>
<li><p>Downloader（下载器）：负责接收引擎传过来的下载请求，然后去网络上下载对应的数据再交还给引擎。</p>
</li>
<li><p>Item Pipeline（管道）：负责将Spider（爬虫）传递过来的数据进行保存。具体保存在哪里，应该看开发者自己的需求。</p>
</li>
<li><p>Downloader Middlewares（下载中间件）：可以扩展下载器和引擎之间通信功能的中间件。</p>
</li>
<li><p>Spider Middlewares（Spider中间件）：可以扩展引擎和爬虫之间通信功能的中间件。</p>
<p><a target="_blank" rel="noopener" href="https://imgtu.com/i/cRzQ1S"><img src="https://z3.ax1x.com/2021/04/16/cRzQ1S.png" alt="cRzQ1S.png" style="zoom:50%;" /></a></p>
</li>
</ol>
<h3 id="创建Scrapy项目："><a href="#创建Scrapy项目：" class="headerlink" title="创建Scrapy项目："></a>创建Scrapy项目：</h3><ol>
<li>创建项目：<code>scrapy startproject [项目名称]</code>.</li>
<li>创建爬虫：<code>cd到项目中-&gt;scrapy genspider [爬虫名称] [域名]</code>.</li>
</ol>
<h3 id="项目文件作用："><a href="#项目文件作用：" class="headerlink" title="项目文件作用："></a>项目文件作用：</h3><ol>
<li><code>settings.py</code>：用来配置爬虫的。</li>
<li><code>middlewares.py</code>：用来定义中间件。</li>
<li><code>items.py</code>：用来提前定义好需要下载的数据字段。</li>
<li><code>pipelines.py</code>：用来保存数据。</li>
<li><code>scrapy.cfg</code>：用来配置项目的。</li>
</ol>
<h3 id="CrawlSpider爬虫："><a href="#CrawlSpider爬虫：" class="headerlink" title="CrawlSpider爬虫："></a>CrawlSpider爬虫：</h3><ol>
<li>作用：可以定义规则，让Scrapy自动的去爬取我们想要的链接。而不必跟Spider类一样，手动的yield Request。</li>
<li>创建：scrapy genspider -t crawl [爬虫名] [域名]</li>
<li>提取的两个类：<ul>
<li>LinkExtrator：用来定义需要爬取的url规则。</li>
<li>Rule：用来定义这个url爬取后的处理方式，比如是否需要跟进，是否需要执行回调函数等。</li>
</ul>
</li>
</ol>
<h3 id="Scrapy-Shell："><a href="#Scrapy-Shell：" class="headerlink" title="Scrapy Shell："></a>Scrapy Shell：</h3><p>在命令行中，进入到项目所在的路径。然后：<br><code>scrapy shell 链接</code><br>在这个里面，可以先去写提取的规则，没有问题后，就可以把代码拷贝到项目中。方便写代码。</p>
<h3 id="使用twisted异步保存mysql数据：（猎云网案例）"><a href="#使用twisted异步保存mysql数据：（猎云网案例）" class="headerlink" title="使用twisted异步保存mysql数据：（猎云网案例）"></a>使用twisted异步保存mysql数据：（猎云网案例）</h3><p>在Lspider中获取网页信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> LieyunItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LieyunSpiderSpider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;lieyun_spider&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;lieyunwang.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://lieyunwang.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;/latest/p\d+\.html&#x27;</span>), follow=<span class="literal">True</span>),</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;/archives/\d+&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">False</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        title_list = response.xpath(<span class="string">&quot;//h1[@class=&#x27;lyw-article-title-inner&#x27;]/text()&quot;</span>).getall()</span><br><span class="line">        title = <span class="string">&quot;&quot;</span>.join(title_list).strip()</span><br><span class="line">        pub_time = response.xpath(<span class="string">&quot;//h1[@class=&#x27;lyw-article-title-inner&#x27;]/span/text()&quot;</span>).get()</span><br><span class="line">        author = response.xpath(<span class="string">&quot;//a[contains(@class,&#x27;author-name&#x27;)]/text()&quot;</span>).get()</span><br><span class="line">        context = response.xpath(<span class="string">&quot;//div[@id=&#x27;main-text-id&#x27;]&quot;</span>).getall()</span><br><span class="line">        item = LieyunItem(title=title, pub_time=pub_time, author=author, context=context, detail_url=response.url)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>在pipeline中，使用twisted.enterprise.adbapi来创建一个连接对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.enterprise <span class="keyword">import</span> adbapi</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LieyunPipeline</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,mysql_config</span>):</span></span><br><span class="line">    self.dbpool = adbapi.ConnectionPool(</span><br><span class="line">        mysql_config[<span class="string">&#x27;DRIVER&#x27;</span>],</span><br><span class="line">        host=mysql_config[<span class="string">&#x27;HOST&#x27;</span>],</span><br><span class="line">        port=mysql_config[<span class="string">&#x27;PORT&#x27;</span>],</span><br><span class="line">        user=mysql_config[<span class="string">&#x27;USER&#x27;</span>],</span><br><span class="line">        password=mysql_config[<span class="string">&#x27;PASSWORD&#x27;</span>],</span><br><span class="line">        db=mysql_config[<span class="string">&#x27;DATABASE&#x27;</span>],</span><br><span class="line">        charset=<span class="string">&#x27;utf8&#x27;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"> <span class="comment"># 重写该方法,获取配置文件中的MYSQL_CONFIG信息，再调用构造器</span></span><br><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">    mysql_config = crawler.settings[<span class="string">&#x27;MYSQL_CONFIG&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> cls(mysql_config)</span><br></pre></td></tr></table></figure>
<p>在插入数据的函数中，使用<code>runInteraction</code>来运行真正执行sql语句的函数。示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">      <span class="comment"># 执行插入操作</span></span><br><span class="line">      result = self.dbpool.runInteraction(self.item_insert, item)</span><br><span class="line">      <span class="comment"># 失败调用</span></span><br><span class="line">      result.addErrback(self.insert_error)</span><br><span class="line">      <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 插入数据，通过游标cursor插入item中的数据</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">item_insert</span>(<span class="params">self, cursor, item</span>):</span></span><br><span class="line">      sql = <span class="string">&quot;insert into lieyun values(null,%s,%s,%s,%s,%s)&quot;</span></span><br><span class="line">      args = (item[<span class="string">&#x27;title&#x27;</span>], item[<span class="string">&#x27;author&#x27;</span>], item[<span class="string">&#x27;detail_url&#x27;</span>], item[<span class="string">&#x27;pub_time&#x27;</span>], item[<span class="string">&#x27;context&#x27;</span>])</span><br><span class="line">      cursor.execute(sql, args)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 输出报错信息</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insert_error</span>(<span class="params">self,fail</span>):</span></span><br><span class="line">      print(<span class="string">&quot;*&quot;</span>*<span class="number">100</span>)</span><br><span class="line">      print(fail)</span><br><span class="line">      print(<span class="string">&quot;*&quot;</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Scrapy下载图片："><a href="#Scrapy下载图片：" class="headerlink" title="Scrapy下载图片："></a>Scrapy下载图片：</h3><ol>
<li>根据网页解析图片的链接。</li>
<li>定义一个item，上面有一定要有两个字段，一个是image_urls，一个是images。其中image_urls是用来存储图片的链接，由开发者把数据爬取下来后添加的，images时存放相关信息的，我们不需要管他</li>
<li>使用scrapy.pipelines.images.ImagesPipeline来作为数据保存的pipeline。</li>
<li>在settings.py中设置IMAGES_SOTRE来定义图片下载的路径。</li>
</ol>
<h4 id="在settings-py这样定义"><a href="#在settings-py这样定义" class="headerlink" title="在settings.py这样定义"></a>在settings.py这样定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="comment"># &#x27;zcool.pipelines.ZcoolPipeline&#x27;: 300,</span></span><br><span class="line">    <span class="comment"># 不用自己的pipeline，用scrapy自带的图片pipeline</span></span><br><span class="line">    <span class="string">&#x27;scrapy.pipelines.images.ImagesPipeline&#x27;</span>: <span class="number">300</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 图片保存路径</span></span><br><span class="line">IMAGES_STORE = os.path.join(os.path.dirname(os.path.dirname(__file__)), <span class="string">&#x27;images&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>如果上面这样写的换，图片路径全在images/full中</p>
<p>如果想要有更复杂的图片保存的路径需求，可以在pipeline中重写ImagePipeline的file_path方法，这个方法用来返回每个图片的保存路径。而<code>file_path</code>方法没有<code>item</code>对象，所以我们还需要重写<code>get_media_requests</code>方法，来把<code>item</code>绑定到<code>request</code>上。注:settings.py中要改回自定义的pipeline</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">from</span> zcool <span class="keyword">import</span> settings</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZcoolPipeline</span>(<span class="params">ImagesPipeline</span>):</span></span><br><span class="line">    <span class="comment"># 将item数据挂载到request中</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span>(<span class="params">self, item, info</span>):</span></span><br><span class="line">        requests = <span class="built_in">super</span>(ZcoolPipeline, self).get_media_requests(item, info)</span><br><span class="line">        <span class="keyword">for</span> request <span class="keyword">in</span> requests:</span><br><span class="line">            request.item = item</span><br><span class="line">        <span class="keyword">return</span> requests</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span>(<span class="params">self, request, response=<span class="literal">None</span>, info=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 获取原来的图片路径</span></span><br><span class="line">        origin_path = <span class="built_in">super</span>(ZcoolPipeline, self).file_path(request, response, info)</span><br><span class="line">        <span class="comment"># 获取标题，并将不合法字符删除</span></span><br><span class="line">        title = request.item[<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">        title = re.sub(<span class="string">r&#x27;\\/:\*\?\|&lt;&gt;&quot;&#x27;</span>, title)</span><br><span class="line">        <span class="comment"># 根据图片的标题创建文件夹</span></span><br><span class="line">        dir_path = os.path.join(settings.IMAGES_STORE, title)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dir_path):</span><br><span class="line">            os.mkdir(dir_path)</span><br><span class="line"></span><br><span class="line">        curr_path = origin_path.replace(<span class="string">&quot;/full&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> os.path.join(dir_path,curr_path)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="下载器中间件："><a href="#下载器中间件：" class="headerlink" title="下载器中间件："></a>下载器中间件：</h2><p>下载器中间件是引擎和下载器之间通信的中间件。在这个中间件中我们可以设置代理、更换请求头等来达到反反爬虫的目的。要写下载器中间件，可以在下载器中实现两个方法。</p>
<ol>
<li>process_request(self,request,spider)：  请求发送之前会执行</li>
<li>process_response(self,request,response,spider)：数据下载到引擎之前执行</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://imgtu.com/i/cRzl6g"><img src="https://z3.ax1x.com/2021/04/16/cRzl6g.png" alt="cRzl6g.png" style="zoom:50%;" /></a></p>
<h3 id="Scrapy中更换请求头：使用fake-useragent-动态更换"><a href="#Scrapy中更换请求头：使用fake-useragent-动态更换" class="headerlink" title="Scrapy中更换请求头：使用fake_useragent 动态更换"></a>Scrapy中更换请求头：使用fake_useragent 动态更换</h3><p>pip install fake_useragent</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment"># 自定义跟换请求头中间件</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserAgentMiddlewares</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        ua = UserAgent()</span><br><span class="line">        <span class="comment"># 更换请求头</span></span><br><span class="line">        request.headers[<span class="string">&#x27;User-Agent&#x27;</span>] = ua.random</span><br></pre></td></tr></table></figure>
<h3 id="Scrapy中设置代理："><a href="#Scrapy中设置代理：" class="headerlink" title="Scrapy中设置代理："></a>Scrapy中设置代理：</h3><ol>
<li><p>设置普通代理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义更换代理中间件</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddlewares</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    proxies = &#123;<span class="string">&quot;code&quot;</span>: <span class="number">0</span>, <span class="string">&quot;data&quot;</span>: [&#123;<span class="string">&quot;ip&quot;</span>: <span class="string">&quot;36.6.146.163&quot;</span>, <span class="string">&quot;port&quot;</span>: <span class="number">4225</span>&#125;, &#123;<span class="string">&quot;ip&quot;</span>: <span class="string">&quot;119.54.15.185&quot;</span>, <span class="string">&quot;port&quot;</span>: <span class="number">4258</span>&#125;],<span class="string">&quot;msg&quot;</span>: <span class="string">&quot;0&quot;</span>, <span class="string">&quot;success&quot;</span>: <span class="literal">True</span>&#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        <span class="comment"># 任意选中一个代理对象</span></span><br><span class="line">        proxy = random.choice(self.proxies[<span class="string">&#x27;data&#x27;</span>])</span><br><span class="line">        <span class="comment"># 更换代理</span></span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>] = <span class="string">&#x27;http://&#x27;</span> + proxy[<span class="string">&#x27;ip&#x27;</span>]+<span class="string">&quot;:&quot;</span>+<span class="built_in">str</span>(proxy[<span class="string">&#x27;port&#x27;</span>])</span><br></pre></td></tr></table></figure></li>
<li><p>设置独享代理：需要提供密钥</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IPProxyDownloadMiddleware</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self,request,spider</span>):</span></span><br><span class="line">        proxy = <span class="string">&#x27;121.199.6.124:16816&#x27;</span></span><br><span class="line">        user_password = <span class="string">&quot;970138074:rcdj35xx&quot;</span></span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>] = proxy</span><br><span class="line">        <span class="comment"># bytes</span></span><br><span class="line">        b64_user_password = base64.b64encode(user_password.encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">        request.headers[<span class="string">&#x27;Proxy-Authorization&#x27;</span>] = <span class="string">&#x27;Basic &#x27;</span> + b64_user_password.decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>代理服务商：</p>
<ul>
<li>芝麻代理：<a target="_blank" rel="noopener" href="http://http.zhimaruanjian.com/">http://http.zhimaruanjian.com/</a></li>
<li>太阳代理：<a target="_blank" rel="noopener" href="http://http.taiyangruanjian.com/">http://http.taiyangruanjian.com/</a></li>
<li>快代理：<a target="_blank" rel="noopener" href="http://www.kuaidaili.com/">http://www.kuaidaili.com/</a></li>
<li>讯代理：<a target="_blank" rel="noopener" href="http://www.xdaili.cn/">http://www.xdaili.cn/</a></li>
<li>蚂蚁代理：<a target="_blank" rel="noopener" href="http://www.mayidaili.com/">http://www.mayidaili.com/</a></li>
<li>极光代理：<a target="_blank" rel="noopener" href="http://www.jiguangdaili.com/">http://www.jiguangdaili.com/</a></li>
</ul>
</li>
</ol>
<h3 id="猎聘网动态更换ip代理案例"><a href="#猎聘网动态更换ip代理案例" class="headerlink" title="猎聘网动态更换ip代理案例"></a>猎聘网动态更换ip代理案例</h3><h4 id="定义一个模型，根据代理ip的json数据，解析里面的代理ip"><a href="#定义一个模型，根据代理ip的json数据，解析里面的代理ip" class="headerlink" title="定义一个模型，根据代理ip的json数据，解析里面的代理ip"></a>定义一个模型，根据代理ip的json数据，解析里面的代理ip</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">model</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, proxy_dict</span>):</span></span><br><span class="line">        proxy_data = proxy_dict[<span class="string">&#x27;data&#x27;</span>][<span class="number">0</span>]</span><br><span class="line">        self.proxy_url = <span class="string">&quot;https://&quot;</span> + proxy_data[<span class="string">&#x27;ip&#x27;</span>] + <span class="string">&quot;:&quot;</span> + <span class="built_in">str</span>(proxy_data[<span class="string">&#x27;port&#x27;</span>])</span><br><span class="line">        self.expire_time = datetime.datetime.strptime(proxy_data[<span class="string">&#x27;expire_time&#x27;</span>], <span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>)</span><br><span class="line">        self.is_backed = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property  </span><span class="comment"># 时间是否过去</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_expire</span>(<span class="params">self</span>):</span></span><br><span class="line">        now = datetime.datetime.now()</span><br><span class="line">        <span class="comment"># 如果超时或还剩5秒就返回True</span></span><br><span class="line">        <span class="keyword">if</span> (self.expire_time - now) &lt; datetime.timedelta(seconds=<span class="number">5</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="在中间件中定义代理ip的动态更换"><a href="#在中间件中定义代理ip的动态更换" class="headerlink" title="在中间件中定义代理ip的动态更换"></a>在中间件中定义代理ip的动态更换</h4><p>process_request : 更换 ip </p>
<p>process_response: 判断返回的网页是否有问题，如果有就标记  is_backed 为True，表示当前ip加入了黑名单</p>
<p>update_proxy：调用模型更新代理ip，将ip信息赋给 self.proxy_model</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> is_item, ItemAdapter</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> proxy_model <span class="keyword">import</span> model</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LiepinDownloaderMiddleware</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LiepinDownloaderMiddleware, self).__init__()</span><br><span class="line">        <span class="comment"># 存放代理ip的相关数据</span></span><br><span class="line">        self.proxy_model = <span class="literal">None</span></span><br><span class="line">        self.proxy_url = <span class="string">&#x27;通过代理服务商获取ip地址&#x27;</span></span><br><span class="line">        self.head = &#123; 。。。 &#125;</span><br><span class="line">        self.update_proxy()  <span class="comment"># 一开始就用代理ip</span></span><br><span class="line">        self.lock = threading.Lock() <span class="comment"># 锁</span></span><br><span class="line">        <span class="comment"># 另外开启一条线程管理ip代理</span></span><br><span class="line">        threading.Thread(target=self.update_proxy_in_thread).start()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>] = self.proxy_model.proxy_url</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span>(<span class="params">self, request, response, spider</span>):</span></span><br><span class="line">        <span class="comment"># 如果状态码步数200，说明该代理ip加入黑名单了，需要标记一下,并直接返回request对象，下次继续加载该请求</span></span><br><span class="line">        <span class="keyword">if</span> response.status != <span class="number">200</span>:</span><br><span class="line">            self.lock.locked()  <span class="comment"># 加锁</span></span><br><span class="line">            self.proxy_model.is_backed = <span class="literal">True</span></span><br><span class="line">            self.lock.release()  <span class="comment"># 释放锁</span></span><br><span class="line">            <span class="keyword">return</span> request</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新代理ip，将ip信息赋给self.proxy_model</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_proxy</span>(<span class="params">self</span>):</span></span><br><span class="line">        proxy_dict = requests.get(self.proxy_url, headers=self.head).json()</span><br><span class="line">        proxy_model = model(proxy_dict)</span><br><span class="line">        self.proxy_model = proxy_model</span><br><span class="line">        print(<span class="string">&quot;更换了代理&quot;</span>)</span><br><span class="line">        print(self.proxy_model.proxy_url)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 多线程的方式更新代理</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_proxy_in_thread</span>(<span class="params">self</span>):</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            time.sleep(<span class="number">5</span>)</span><br><span class="line">            <span class="comment"># 每过5秒检查一次，如果代理 运行了60秒或 加入黑名单或 超时，就更换代理</span></span><br><span class="line">            <span class="keyword">if</span> count &gt;= <span class="number">60</span> <span class="keyword">or</span> self.proxy_model.is_backed <span class="keyword">or</span> self.proxy_model.is_expire:</span><br><span class="line">                self.update_proxy()</span><br><span class="line">                count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                count = count + <span class="number">5</span></span><br><span class="line">                print(<span class="string">&quot;代理运行了&quot;</span> + <span class="built_in">str</span>(count) + <span class="string">&quot;秒&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Scrapy中使用selenium动态爬虫"><a href="#Scrapy中使用selenium动态爬虫" class="headerlink" title="Scrapy中使用selenium动态爬虫"></a>Scrapy中使用selenium动态爬虫</h2><p>在中间件的request中使用selenium获取网页数据，直接返回给爬虫进行解析数据</p>
<h3 id="简书案例-获取加载更多中的全部数据"><a href="#简书案例-获取加载更多中的全部数据" class="headerlink" title="简书案例,获取加载更多中的全部数据"></a>简书案例,获取加载更多中的全部数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> is_item, ItemAdapter</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> scrapy.http.response.html <span class="keyword">import</span> HtmlResponse</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JianshuDownloaderMiddleware</span>:</span></span><br><span class="line">    <span class="comment"># 初始化dirver</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(JianshuDownloaderMiddleware, self).__init__()</span><br><span class="line">        self.driver = webdriver.Chrome(<span class="string">&quot;C:\project\pyCode\chromedriver.exe&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拦截请求，自定义发送动态爬虫，直接返回response对象给爬虫</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        self.driver.get(request.url)</span><br><span class="line">        <span class="comment"># 等待加载</span></span><br><span class="line">        WebDriverWait(self.driver, <span class="number">8</span>).until(</span><br><span class="line">            EC.element_to_be_clickable((By.XPATH, <span class="string">&quot;//section[position()=2]/h3&quot;</span>))</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 循环点击加载更多，直到加载完毕即可</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                add_more = self.driver.find_element_by_xpath(<span class="string">&quot;//section[position()=2]/div/div[@role=&#x27;button&#x27;]&quot;</span>)</span><br><span class="line">                print(add_more)</span><br><span class="line">                self.driver.execute_script(<span class="string">&quot;arguments[0].click();&quot;</span>, add_more)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        response = HtmlResponse(request.url, request=request, body=self.driver.page_source,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure>
<p>此时爬虫spider中获取的response是从上面这个方法中传过来的，可以根据自己的需求获取相关数据</p>
<p>更多的介绍请参考API：<a target="_blank" rel="noopener" href="https://scrapyd.readthedocs.io/en/stable/api.html">https://scrapyd.readthedocs.io/en/stable/api.html</a></p>
<p>over。。。</p>

    </div>

    
    
    
        <div class="reward-container">
  <div>感谢老铁们的支持!!!</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/weChatPay.png" alt="怪狗狗 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/aliPay.jpg" alt="怪狗狗 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/python/" rel="tag"> python</a>
              <a href="/tags/scrapy/" rel="tag"> scrapy</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/04/12/%E5%8F%8D%E7%88%AC%E8%99%AB/" rel="prev" title="反爬虫">
      <i class="fa fa-chevron-left"></i> 反爬虫
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/04/18/pandas%E5%BA%93/" rel="next" title="pandas库">
      pandas库 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy%E6%A1%86%E6%9E%B6%EF%BC%9A"><span class="nav-number">1.</span> <span class="nav-text">Scrapy框架：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85Scrapy%E6%A1%86%E6%9E%B6%EF%BC%9A"><span class="nav-number">1.1.</span> <span class="nav-text">安装Scrapy框架：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy%E6%A1%86%E6%9E%B6%E6%9E%B6%E6%9E%84%EF%BC%9A"><span class="nav-number">1.2.</span> <span class="nav-text">Scrapy框架架构：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BAScrapy%E9%A1%B9%E7%9B%AE%EF%BC%9A"><span class="nav-number">1.3.</span> <span class="nav-text">创建Scrapy项目：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E6%96%87%E4%BB%B6%E4%BD%9C%E7%94%A8%EF%BC%9A"><span class="nav-number">1.4.</span> <span class="nav-text">项目文件作用：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CrawlSpider%E7%88%AC%E8%99%AB%EF%BC%9A"><span class="nav-number">1.5.</span> <span class="nav-text">CrawlSpider爬虫：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy-Shell%EF%BC%9A"><span class="nav-number">1.6.</span> <span class="nav-text">Scrapy Shell：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8twisted%E5%BC%82%E6%AD%A5%E4%BF%9D%E5%AD%98mysql%E6%95%B0%E6%8D%AE%EF%BC%9A%EF%BC%88%E7%8C%8E%E4%BA%91%E7%BD%91%E6%A1%88%E4%BE%8B%EF%BC%89"><span class="nav-number">1.7.</span> <span class="nav-text">使用twisted异步保存mysql数据：（猎云网案例）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87%EF%BC%9A"><span class="nav-number">1.8.</span> <span class="nav-text">Scrapy下载图片：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9C%A8settings-py%E8%BF%99%E6%A0%B7%E5%AE%9A%E4%B9%89"><span class="nav-number">1.8.1.</span> <span class="nav-text">在settings.py这样定义</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E5%99%A8%E4%B8%AD%E9%97%B4%E4%BB%B6%EF%BC%9A"><span class="nav-number">2.</span> <span class="nav-text">下载器中间件：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy%E4%B8%AD%E6%9B%B4%E6%8D%A2%E8%AF%B7%E6%B1%82%E5%A4%B4%EF%BC%9A%E4%BD%BF%E7%94%A8fake-useragent-%E5%8A%A8%E6%80%81%E6%9B%B4%E6%8D%A2"><span class="nav-number">2.1.</span> <span class="nav-text">Scrapy中更换请求头：使用fake_useragent 动态更换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy%E4%B8%AD%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86%EF%BC%9A"><span class="nav-number">2.2.</span> <span class="nav-text">Scrapy中设置代理：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8C%8E%E8%81%98%E7%BD%91%E5%8A%A8%E6%80%81%E6%9B%B4%E6%8D%A2ip%E4%BB%A3%E7%90%86%E6%A1%88%E4%BE%8B"><span class="nav-number">2.3.</span> <span class="nav-text">猎聘网动态更换ip代理案例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%EF%BC%8C%E6%A0%B9%E6%8D%AE%E4%BB%A3%E7%90%86ip%E7%9A%84json%E6%95%B0%E6%8D%AE%EF%BC%8C%E8%A7%A3%E6%9E%90%E9%87%8C%E9%9D%A2%E7%9A%84%E4%BB%A3%E7%90%86ip"><span class="nav-number">2.3.1.</span> <span class="nav-text">定义一个模型，根据代理ip的json数据，解析里面的代理ip</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9C%A8%E4%B8%AD%E9%97%B4%E4%BB%B6%E4%B8%AD%E5%AE%9A%E4%B9%89%E4%BB%A3%E7%90%86ip%E7%9A%84%E5%8A%A8%E6%80%81%E6%9B%B4%E6%8D%A2"><span class="nav-number">2.3.2.</span> <span class="nav-text">在中间件中定义代理ip的动态更换</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy%E4%B8%AD%E4%BD%BF%E7%94%A8selenium%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB"><span class="nav-number">3.</span> <span class="nav-text">Scrapy中使用selenium动态爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E4%B9%A6%E6%A1%88%E4%BE%8B-%E8%8E%B7%E5%8F%96%E5%8A%A0%E8%BD%BD%E6%9B%B4%E5%A4%9A%E4%B8%AD%E7%9A%84%E5%85%A8%E9%83%A8%E6%95%B0%E6%8D%AE"><span class="nav-number">3.1.</span> <span class="nav-text">简书案例,获取加载更多中的全部数据</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="怪狗狗"
      src="/images/jerry.jpg">
  <p class="site-author-name" itemprop="name">怪狗狗</p>
  <div class="site-description" itemprop="description">踏上新征程----go！！！</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">57</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">87</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/oddDog-git" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;oddDog-git" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://1764501567@qq.com/" title="E-Mail → https:&#x2F;&#x2F;1764501567@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://wpa.qq.com/msgrd?v=3&uin=1764501567&site=qq&menu=yes" title="http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;1764501567&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener" target="_blank">加我QQ</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">怪狗狗</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div> -->

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  















  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'YVPmrsyUdC4uV9bapDgb8ll6-gzGzoHsz',
      appKey     : 'MUI57EQtRoMQjLvpW68kiNyE',
      placeholder: "快来评论吧！！！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
